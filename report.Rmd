---
title: "Exploratory Data Analysis and Modeling: Milestone Report"
author: "Prabhat Kumar"
date: "18 March 2016"
output: 
  html_document: 
    number_sections: yes
    toc: yes
---

# Executive Summary
The goal of this project is just to display that I've gotten used to working with the [SwiftKey, Inc.](https://swiftkey.com/en/) data and that I'm on track to create the prediction algorithm. I will explain my exploratory analysis and my goals for the eventual app and algorithm.

The objective of this project is to: (1). Demonstration of how to download the data and have successfully loaded it in, (2). Create a basic report of summary statistics about the data sets, (3). Report any interesting findings that I amassed so far, and (4). Get feedback on my plans for creating a prediction algorithm and Shiny app.

# Data Acquisition
I have acquired the data from given URL, [Coursera-SwiftKey.zip](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip). Further, unzipped the data in to ```data``` directory, by help of bash command ```unzip``` <i>i.e.</i>, is used for opening the archive; ```unzip("./data/Coursera-SwiftKey.zip")```.

```{r Environment Cleaning.}
# Remove Objects from a Specified Environment.
rm(list = ls())
```

# Reading the SwiftKey Data
I'm using here ```readLines``` method, with ```encoding = "UTF-8", skipNul = TRUE``` parameters to load/read en_US data sets, <i>i.e.</i>, en_US.blogs.txt/en_US.news.txt/en_US.twitter.txt.

```{r Reading the SwiftKey Data.}
enBlogs   <- readLines("./data/final/en_US/en_US.blogs.txt",   encoding = "UTF-8", skipNul = TRUE)
enNews    <- readLines("./data/final/en_US/en_US.news.txt",    encoding = "UTF-8", skipNul = TRUE)
enTwitter <- readLines("./data/final/en_US/en_US.twitter.txt", encoding = "UTF-8", skipNul = TRUE)
```

# Cleaning the SwiftKey Data
After, loading the data, I did cleaning of the acquired data. For this work, I've used [stringi](http://www.rexamine.com/resources/stringi/) R library.

Note: Upon examination, there is some removal of non english characters to need to be done to make the data more usable.

```{r Cleaning the SwiftKey Data.}
# working with stringi:
# http://www.rexamine.com/resources/stringi/
# Author: Marek Gagolewski, gagolews@rexamine.com/
library(stringi)

# some removal of non english characters to need to be done to make the data more usable.
# drop non UTF-8 characters.
blogs   <- iconv(enBlogs,   from = "latin1", to = "UTF-8", sub = "")
news    <- iconv(enNews,    from = "latin1", to = "UTF-8", sub = "")
twitter <- iconv(enTwitter, from = "latin1", to = "UTF-8", sub = "")

# Replace Occurrences of a Pattern.
twitter <- stri_replace_all_regex(twitter, "\u2019|`","'")
twitter <- stri_replace_all_regex(twitter, "\u201c|\u201d|u201f|``",'"')

# Save a single object to file.
saveRDS(blogs,   "./enBlogs.rds")
saveRDS(news,    "./enNews.rds")
saveRDS(twitter, "./enTwitter.rds")

# A data frame is a list of variables of the same number of rows with unique row names,
# given class "data.frame". If no variables are included,
# the row names determine the number of rows.
data.frame(blogs = length(blogs), news = length(news), twitter = length(twitter))
```

# Summary Statistics
I have performed Summary Statistics for three files only - en_US.blogs.txt, en_US.news.txt, and en_US.twitter.txt.
```{r Summary Statistics.}
# Summary Statistics for the 3 files - en_US.blogs/en_US.news/en_US.twitter
blogwords    <- sum(stri_count_words(blogs))
newswords    <- sum(stri_count_words(news))
twitterwords <- sum(stri_count_words(twitter))
# Combine Values into a Vector or List,
# This is a generic function which combines its arguments.
words        <- c(blogwords, newswords, twitterwords)

bloglines    <- length(blogs)
newslines    <- length(news)
twitterlines <- length(twitter)
# Combine Values into a Vector or List,
# This is a generic function which combines its arguments.
lines        <- c(bloglines, newslines, twitterlines)

blogmaxc     <- max(nchar(blogs))
newsmaxc     <- max(nchar(news))
twittermaxc  <- max(nchar(twitter))
# Combine Values into a Vector or List,
# This is a generic function which combines its arguments.
maxchars     <- c(blogmaxc, newsmaxc, twittermaxc)

blogmaxw     <- max(stri_count_words(blogs))
newsmaxw     <- max(stri_count_words(news))
twittermaxw  <- max(stri_count_words(twitter))
# Combine Values into a Vector or List,
# This is a generic function which combines its arguments.
maxwords     <- c(blogmaxw, newsmaxw, twittermaxw)

FileSumm     <- data.frame("File Name" = c("en_US.blogs", "en_US.news", "en_US.twitter"),
                           NumberLines = lines,
                           NumberWords = words,
                           MaxChars = maxchars,
                           MaxWords = maxwords)
```

Also, I did some basic descriptive measures of the size of the text.

```{r Basic descriptive measures.}
# Basic descriptive measures of the size of the text.
# working with knitr:
library(knitr)

basic.measures <- cbind(c("Text Chunks", "Characters"),
                  rbind(c(length(blogs),length(news),length(twitter)),
                        c(sum(nchar(blogs)),sum(nchar(news)),sum(nchar(twitter)))))
colnames(basic.measures) <- c("Measure", "Blogs", "News", "Twitter")

kable(basic.measures)
```

# Data Sampling
Because the files are so large, I will take only a random <b>05% sample of the data</b>, from all three data sets. This will decrease overall computation time and allow for quicker experimentation with the data.

```{r Sampling the Data.}
# Sampling the SwiftKey Data.
# Setting the seed for reproducibility.
set.seed(100)

# This function takes a random sample of the data.
sampler <- function(chunk, percent) {
  percent <- round(length(chunk)*percent)
  sample.index <- sample(1:length(chunk), percent)
  
  return(chunk[sample.index])
}

# Let's start off with a 05% sample data.
#                       -----
# //1-US.blogs//
sampleBlogs <- sampler(blogs, .05)
# Write Lines to a Connection.
writeLines(c(sampleBlogs), "./sampleBlogs.txt")

# //2-US.news//
sampleNews <- sampler(news, .05)
# Write Lines to a Connection.
writeLines(c(sampleNews), "./sampleNews.txt")

# //3-US.twitter//
sampleTwitter <- sampler(twitter, .05)
# Write Lines to a Connection.
writeLines(c(sampleTwitter), "./sampleTwitter.txt")

```

<b>Note:</b> Let’s take a look at some basic descriptive measures of the size of the text.

```{r Basic descriptive measures of sample text.}
# Basic descriptive measures of the size of sample text.
# working with knitr:
library(knitr)

basic.measures.1 <- cbind(c("Text Chunks", "Characters"),
                  rbind(c(length(sampleBlogs),length(sampleNews),length(sampleTwitter)),
                        c(sum(nchar(sampleBlogs)),sum(nchar(sampleNews)),sum(nchar(sampleTwitter)))))
colnames(basic.measures.1) <- c("Measure", "Blogs", "News", "Twitter")

kable(basic.measures.1)
```

# Profanity Filtering
Now, this sampled data set is quite good and likely has a few things I can take out. For starters, [profanity filtering](https://en.wikipedia.org/wiki/Wordfilter) is a good idea. I will reformat and combine two profanity lists first. Then, I will take out any text chunks that contain anything on the list.

<b>Note:</b> The lists used can be found here - <u>[1](https://github.com/iamprabhat/capstone/blob/master/data/Bad_Words_1.txt)</u> &amp; <u>[2](https://github.com/iamprabhat/capstone/blob/master/data/Bad_Words_2.txt)</u>.

```{r Profanity Filtering of SwiftKey Sample Data.}
# Profanity Filtering of SwiftKey Sample Data.
# First list of bad words:------------------------
bad.words.1 <- readLines("./data/Bad_Words_1.txt")
bad.words.1 <- bad.words.1[-length(bad.words.1)]

# Second list of bad words:-----------------------
bad.words.2 <- readLines("./data/Bad_Words_2.txt")
bad.words.2 <- bad.words.2[-1]

bad.words.2 <- substr(x = bad.words.2, start = 1, stop = nchar(bad.words.2)-3)
double.quote.index <- grep(pattern = "\"", x = bad.words.2)

bad.words.2[double.quote.index] <- substr(x = bad.words.2[double.quote.index], start = 2, 
                                          stop = nchar(bad.words.2[double.quote.index])-1)

all.bad.words <- c(bad.words.1, bad.words.2)
all.bad.words <- unique(all.bad.words)
all.bad.words <- paste(all.bad.words, collapse="|")
all.bad.words <- substr(x = all.bad.words, start = 1, stop = nchar(all.bad.words)-1)

bad.words.blogs   <- grep(all.bad.words, sampleBlogs)
bad.words.news    <- grep(all.bad.words, sampleNews)
bad.words.twitter <- grep(all.bad.words, sampleTwitter)

bad.prop.blogs    <- length(bad.words.blogs)/length(sampleBlogs)
bad.prop.news     <- length(bad.words.news)/length(sampleNews)
bad.prop.twitter  <- length(bad.words.twitter)/length(sampleTwitter)

sampleBlogs   <- sampleBlogs[-bad.words.blogs]
sampleNews    <- sampleNews[-bad.words.news]
sampleTwitter <- sampleTwitter[-bad.words.twitter]
```

<b>Note:</b> Now, let’s take a look at the size of the new data set and what percentage of the chunks had profanities.

```{r percentage of the chunks has profanities.}
# What percentage of the chunks has profanities?
# ˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆ
basic.measures.2 <- cbind(c("Text Chunks", "Characters", "Profanity Percent"),
                    rbind(c(length(sampleBlogs),length(sampleNews),length(sampleTwitter)),
                          c(sum(nchar(sampleBlogs)),sum(nchar(sampleNews)),sum(nchar(sampleTwitter))),
                          round(c(bad.prop.blogs,bad.prop.news,bad.prop.twitter), 4)))
colnames(basic.measures.2) <- c("Measure", "Blogs", "News", "Twitter")

kable(basic.measures.2)
```
